---
title: "Online supplement for the paper 'Can recall data be trusted?'"
author: "Michael Daniel, Alexey Koshevoy, Ilya Schurov, and Nina Dobrushina"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
bibliography: biblio.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(rstudioapi)
library(dplyr)
library(sjPlot)
library(grid)
library(randomForest)
library(shiny)
library(ggrepel)
library(tidyverse)
library(kableExtra)
library(nomnoml)
# current_path <- getActiveDocumentContext()$path 
# setwd(dirname(current_path))
```

## Pipeline 

```{r, echo=FALSE, message=FALSE}
nomnoml::nomnoml("[Inital dataset|
  [Type]->[Direct]
  [Type]->[Indirect]]
[Inital dataset|
  [Type]->[Direct]
  [Type]->[Indirect]] -> [Indirect data]
[Inital dataset|
  [Type]->[Direct]
  [Type]->[Indirect]] -> [Direct data]
[Direct data]->[Train on direct]
[Indirect data]->[Train on indirect]


[All possible 
combinations|year of birth; sex; village]->[Trained on direct]
[All possible 
combinations|year of birth; sex; village]->[Trained on indirect]
[Trained on direct]-> prediction[subtract]
[Trained on indirect]-> [subtract]
[subtract]->[imputed true difference]


[All possible 
combinations|year of birth; sex; village]-> shuffle data type[data]
[data]->[diff]
[diff]->[imputed true difference]
[imputed true difference]->[Get distribution]")
```


## Data visualizations
### Average ITM in different villages

```{r, echo=FALSE, message=FALSE}
data <- read.csv('data/all.csv')
data[data$type == 1,]$type <-  'Direct'
data[data$type == 0,]$type <-  'Indirect'
data$born <-  0
data[data$year_of_birth <= 1922,]$born <-  'Before 1922'
data[data$year_of_birth >= 1922,]$born <-  'After 1922'
levels(data$sex) <- c('Female', 'Male')
new <- dplyr::select(data, residence, number.of.lang.strat, born, village.population)
new <- group_by(new, residence, born, village.population)
new_c <- new %>% summarise(mean=mean(number.of.lang.strat), count=n())
new_c$mean_count <- 0
new_c[new_c$born == 'After 1922',]$mean_count <- mean(new_c[new_c$born == 'After 1922',]$count)
new_c[new_c$born == 'Before 1922',]$mean_count <- mean(new_c[new_c$born == 'Before 1922',]$count)
new_c$mean_itm <- 0
new_c[new_c$born == 'After 1922',]$mean_itm <- mean(new_c[new_c$born == 'After 1922',]$mean)
new_c[new_c$born == 'Before 1922',]$mean_itm <- mean(new_c[new_c$born == 'Before 1922',]$mean)
new_c$born_t <- 0
new_c$born_t <-  factor(new_c$born, levels=c('Before 1922','After 1922'))

ggplot(new_c, aes(x=mean, y=count))+
  geom_point(aes(size=village.population))+
  # geom_point()+
  labs(y = "Number of observations", x='Mean ITM', size=' Village\n population')+
  facet_grid(cols=vars(born))+
  # geom_text_repel(aes(label=residence),hjust=0, vjust=0)+
  theme_bw()+
  theme(legend.position=c(1,1),legend.justification=c(1,1),
        legend.direction="vertical",
        legend.box="horizontal",
        legend.box.just = c("top"), 
        legend.background = element_rect(fill=alpha('transparent', 0)))
```


Average ITM is shown for two age groups, people born before (exclusive) and after (inclusive) 1922, in (a) and (b), respectively. Comparing specific locations before and after 1922^[Year 1922 is the first year for which we have two datapoints obtained directly, from the people born in this year. It is chosen as a cutoff point with a view to further analysis (see section The model in the paper)], as well as the total average (the horizontal dotted red line), shows a decrease in the average ITM values. This reflects the loss of traditional multilingualism in the course of the last century, as it was being ousted by the use of Russian as lingua franca [@dobrushina2019gendered]. In the more traditional configuration, on the left panel, one can see that an ‘average Daghestanian villager’ was likely to know at least one language as L2 (not to count Russian). Hinuq, the village speaking Hinuq, a one-village language of the Tsezic branch, shows one of the highest values of ITM in both charts. Average number of L2 for a resident of Hinuq is 3.5 for people in the first group, and about 2.7 in the second age group. On the other end of the scale, villages of Darvag (Azerbaijani), Dorgeli (Kumyk) and Durangi (Avar) show no traditional multilingualism at all. 


### Percentage of direct data and the total number of observations across villages

```{r, echo=FALSE, message=FALSE}
data_t <- read.csv('data/all.csv')
new <- dplyr::select(data_t, residence, type, village.population)
new <- group_by(new, residence, village.population)
new_type <- new %>% summarise(mean=mean(type), count=n())
ggplot(new_type, aes(x=count, y=mean))+
  geom_point()+
  # geom_point()+
  labs(y = "Percentage of direct data", x='Number of observations', size=' Village\n population', color='Born')+
  # facet_grid(cols=vars(born))+
  geom_text_repel(aes(label=residence))+
  theme_bw()+
  scale_y_continuous(labels = scales::percent)+
  geom_hline(color='red', yintercept = mean(new_type$mean), linetype='dashed')+
  geom_vline(color='red', xintercept = mean(new_type$count), linetype='dashed')
```


Dots are villages. Axis Y indicates the amount of the direct data obtained for the village as a percentage to all data points obtained for this village. Axis X indicates the total number of datapoints obtained for the village. The dotted lines show the average values. As the figure above shows, the amount of data per village is usually between 50 and 150, and the percentage of direct data usually lies between 20 and 40. 

## Model tuning and selection
Here we provide a summary of our model selection process. See full details and code in file **tuning-final.ipynb**.

### Selection process
We treat prediction of Russian as two-class classification problem and prediction of ITM as a regression problem.

For Russian, we consider gradient boosting models (`GradientBoostringClassifier` from *scikit-learn* library and `CatBoostClassifier` from *catboost* project), random forest (`RandomForestClassifier` from *scikit-learn*) and logistic regression (`LogisticRegression` from *scikit-learn*).

For ITM, we also consider gradient boosting (`GradientBoostringRegressor` and `CatBoostRegressor`) and random forest (`RandomForestRegressor`) as well as linear regression (`LinearRegression`). 

We avoid regularization in linear and logistic regression as we want unbaised estimations of the target variable.

Categorical variables (`sex`, `mother toungue` and `residence`) are processed with *one hot* encoding scheme for all models except *catboost* (these models has their own ways to deal with categorical variables).

First, we split dataset into two parts: with direct and indirect data. We perform optimization on each part separately.

As usual, each part of the dataset is splitted into train and test sample (70%/30%). Hyperparameter selection is performed using grid search with 10-fold cross validation on the train set. Then the best model is evaluated on the test set.

The optimization objective negative log-loss (a.k.a. negative cross-entropy or log-likelihood) for Russian and $R^2$ for ITM because they provide unbaised estimate for the corresponding mean.

### Model comparison

Here we provide scores of the best model of each class.

#### ITM ($R^2$)

```{r, echo=FALSE, message=FALSE}
itm_res <- read_csv("itm_res_piv.csv")
colnames(itm_res) <- c('Estimator', 'Indirect', 'Direct')
itm_res %>% 
  kable() %>% 
  kable_styling("hover", full_width = FALSE) 
```
#### Russian (log-likelihood)
```{r, echo=FALSE, message=FALSE}
rus_res <- read_csv("rus_res_piv.csv")
colnames(rus_res) <- c('Estimator', 'Indirect', 'Direct')
rus_res %>%
  kable() %>% 
  kable_styling("hover", full_width = FALSE) 
```
#### Model selection and scores of best models
As we can see from the tables above, gradient boosting models (*scikit-learn*'s `GradientBoosting*` and `CatBoost*`) outperform the other models in both settings, though advantage over `RandomForest` is rather small (but seem to be consistent according to our experiments). When choosing between `CatBoost*` and `GradientBoosting*` models, we prefer the latter ones because they are faster.

Below we provide scores of best models. For Russian prediction, we report accuracy and F-measure as goodness-of-fit measures.

```{r, echo=FALSE, message=FALSE}
scores <- (read.csv("scores.csv", skip = 1) %>% 
             slice(2:n()))
colnames(scores) <- c('type', '$R^2$', 'Accuracy', 'F-measure')
scores$type <-  ifelse(scores$type == 0, "Indirect", "Direct")
scores %>% 
  mutate_if(is.numeric, format, digits=5,nsmall = 0) %>% 
  kable(align="c") %>% 
  add_header_above(c(" " = 1, "ITM" = 1, "Russian" = 2)) %>%
  kable_styling("hover", full_width = FALSE) 
```
## Alternative approaches
These alternative approaches were suggested by the anonymous referees.

### Linear and logistic regressions with random effects
Here we try classical approach that uses (generalized) linear regression models. We fit the model to the whole dataset (direct and indirect data combined), but include variable `type` (data type) as one of the predictors. The coefficient of this predictor measures the difference between the expected value of target variable all other variables being equal. To take into account possible village-specific effects, we use `lme4` library (Bates, Maechler, Bolker, Walker 2015) and include `village` as random effect to our models.

```{r, echo=FALSE, message=FALSE}
library(lme4)
ITM <- read.csv("data/ITM.csv")
russian <- read.csv("data/russian.csv")
```

#### ITM
For ITM target variable we use multivariate linear regression with village-specific random intercept.

```{r}
fit <- lmer(number.of.lang ~ year_of_birth + type + sex + (1|residence), data=ITM)
summary(fit)
```

The reported estimate for `type` coefficient is `r summary(fit)$coeff['type', 'Estimate']`, but comparing it with the corresponding Std. Error , we conclude that this effect is not statistically significant. This is in agreement with the results of our paper.

#### Russian
For Russian target variable (that is binary) we use multivariate logistic regression with village-specific random intercept.

```{r}
fit <- glmer(русский ~ year_of_birth_adj + type + sex + (1|residence),
                    family=binomial(), 
                    data=russian %>% 
                      mutate(year_of_birth_adj = year_of_birth - 1940),
                    # adjust origin, otherwise model does not converge
                    control=glmerControl(optimizer="bobyqa",
                                         optCtrl=list(maxfun=100000)))
summary(fit)
```
The reported estimate for `type` coefficient is `r summary(fit)$coeff['type', 'Estimate']`, but comparing it with the corresponding Std. Error, we conclude that this effect is not statistically significant.

This result differs from the result of our paper, but does not contradict it: absence of statistically significant effect does not imply that there is no effect at all, it merely shows that a particular test cannot detect it from our data. Thus the difference of results can be explained by smaller sensitivity of the generalized linear model, compared with our approach, probably, due to violations of the assumptions of this model.

### Propensity score matching
#### Introduction
Here we present a summary of alternative analysis of our data, using technique, known as propensity score matching (Rosenbaum and Rubin 1983). See full details and the code in file **propensity-score-matching.Rmd**.

To compare target variables (i.e. `ITM` and `Russian`) between direct and indirect data, we have to take into account that chances to obtain observation in direct or indirect data depend on the values of other variables of this observation (most notably, on `year_of_birth`). These variables can also affect the target variables (i.e. ITM decreases with time) and thus produce bias in our estimates (effect known as *confounding*). To adjust for this bias, in the main paper we compare predictions of machine learning models instead of comparing values of target variables directly. This is similar to adjustment procedure used in econometrics, where potential confounders are added into the regression models to "isolate" causal effect of the variable we are interested in (in our case, data type).

Propensity score matching is another approach to deal with this problem. It is a two-step procedure. At the first step, we construct a predictive model that estimates probability for an observation to be included in the indirect subsample, based on the values of other variables. At the second step, we *match* every item in the direct data with some element in the indirect data in such a way that both items, according to values of other variables and our model, has the same probability to be included in the indirect data. This allows us to produce balanced dataset and then compare variables of interest directly.

We will use `MatchIt` library (Ho, Imai, King and Stuart 2011) to make the actual matching. Due to non-linear nature of our data, we use random forest algorithm to estimate the probability at the first step instead of the logistic regression (which is the default).

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(MatchIt)

# type == 1 means direct data, type == 0 otherwise
set.seed(1)
mod_match_ITM <- matchit(type ~ year_of_birth 
                         + language.population 
                         + elevation 
                         + village.population
                         + mother.tongue
                         + residence
                         + sex,
                         method = "nearest",
                         distance = "randomforest",
                         data=ITM,
                         discard='both')
dta_m_ITM <- match.data(mod_match_ITM)
```

#### ITM

After the matching, we can compare the difference in ITM values between direct and indirect data. In fact, this difference is not equivalent to the systematic bias discussed in the paper. What we obtain here is so-called ATT, *average treatment effect on the treated*, i.e. it measures average difference in ITM between direct and indirect data for informants who are "treated", i.e. belong to indirect data subsample. This produce slightly different weighting compared with the method used in paper, where all possible combinations of values of variables were used with equal weights to estimate the difference.

Nevertheless, ATT is also a reasonable estimate of the difference between direct and indirect data, and should produce similar results. To test for statistical significance, we will use `t.test`.

```{r}
t.test(number.of.lang ~ type, data = dta_m_ITM)
```
We see that the difference in ITM between direct and indirect data is statistically insignificant and the difference between groups lies inside of confidence interval (-0.05, 0.04). This is in agreement with the results of our paper.

#### Russian

```{r, echo=FALSE, message=FALSE}
mod_match_russian <- matchit(type ~ year_of_birth 
                         + language.population 
                         + elevation 
                         + village.population
                         + mother.tongue
                         + residence
                         + sex,
              method = "nearest", 
              distance = "randomforest",
              data = russian)
dta_m_russian <- match.data(mod_match_russian)
```

To estimate the difference in Russian, we will use bivariate logistic regression on matched dataset.
```{r}
fit <- glm(
  русский ~ type,
  family=binomial(link='logit'), 
  data=dta_m_russian)
summary(fit)
```
We see that `type` is significant and switching from indirect data (type equals 0) to direct data (type equals 1) increases chances of knowledge of Russian. This is in agreement with the result in the paper and the coefficient lies inside the confidence interval (0.21, 0.88) reported in the paper (the sign is opposite to the reported in the paper as we are denoting systematic bias as "indirect data minus direct data").

#### Conclusion

The results of analysis based on propensity score matching is in agreement with results that is based on comparing predictions between two machine learning models. This demonstrates robustness of our methods.

### References 
