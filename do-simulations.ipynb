{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do simulations\n",
    "In this notebook we use Gradient Boosting models with hyperparameters obtained in **tuning-final.ipynb** to obtain *imputed true difference* and the distributions that allows us to test statistical hypothesis and find confidence intervals discussed in the paper.\n",
    "\n",
    "Two kind of simulations are performed:\n",
    "\n",
    "1. Permutation based simulations that are described in the paper. It allows to construct null distribution and test the hypothesis that obtained value of systematic bias is significant (or not).\n",
    "2. Bootstrap based simulations that allows to construct confidence intervals for systematic bias. In this case instead of permutation we perform sampling with replacement of our initial data, then re-train our models and record their predictions.\n",
    "\n",
    "The results of this notebook are several csv files that a processed in **make-pics.ipynb**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from itertools import chain\n",
    "import seaborn as sns\n",
    "import ray\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from indirect_utils import (\n",
    "    DispatchEstimator,\n",
    "    fullspace,\n",
    "    generate_x_y,\n",
    "    get_delta,\n",
    "    logodds,\n",
    "    stratified_permute,\n",
    "    tologodds,\n",
    "    trimmed,\n",
    "    identity,\n",
    "    residence_info,\n",
    "    read_data,\n",
    "    russian_to_target,\n",
    ")\n",
    "import random\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ITM = read_data(\"data/ITM.csv\")\n",
    "data_russian = read_data(\"data/russian.csv\").rename(columns={\"русский\": \"russian\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = (\n",
    "    fullspace(data_ITM, [\"type\", \"sex\", \"residence\", \"year_of_birth\"])\n",
    "    .merge(residence_info, on=\"residence\", how=\"left\")\n",
    "    .sort_values([\"year_of_birth\", \"type\", \"sex\", \"residence\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-02 17:40:43,651\tINFO resource_spec.py:204 -- Starting Ray with 16.21 GiB memory available for workers and up to 8.11 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2021-03-02 17:40:43,892\tWARNING services.py:923 -- Redis failed to start, retrying now.\n",
      "2021-03-02 17:40:44,151\tINFO services.py:1163 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.201.25',\n",
       " 'raylet_ip_address': '192.168.201.25',\n",
       " 'redis_address': '192.168.201.25:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2021-03-02_17-40-43_649405_645373/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-03-02_17-40-43_649405_645373/sockets/raylet',\n",
       " 'webui_url': 'localhost:8265',\n",
       " 'session_dir': '/tmp/ray/session_2021-03-02_17-40-43_649405_645373'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need ray library to perform parallel training\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_data(data):\n",
    "    \"\"\"\n",
    "    Do sampling with replacement\n",
    "    \"\"\"\n",
    "    return (\n",
    "        data.groupby(\"type\")\n",
    "        .apply(lambda x: x.sample(frac=1, replace=True))\n",
    "        .reset_index(drop=True)\n",
    "        .sort_values([\"year_of_birth\", \"type\"])\n",
    "        .reset_index(drop=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def predict_de(\n",
    "    data,\n",
    "    prediction_space,\n",
    "    estimators,\n",
    "    ct,\n",
    "    russian,\n",
    "    permute,\n",
    "    permute_strats=6,\n",
    "    delta=0,\n",
    "    seed=None,\n",
    "    bootstrap=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    predict with DispatchEstimator\n",
    "\n",
    "    == Params ==\n",
    "    - data: data to train\n",
    "    - prediction_space: values to predict on\n",
    "    - estimators: a pair of base estimators to construct DispatchEstimator\n",
    "    - ct: ColumnTransformer; make sure that first column is type\n",
    "    - russian: bool: will we predict Russian (otherwise ITM)\n",
    "    - permute: should we permute type before training\n",
    "    - permute_strats: number of strats to permute\n",
    "    - delta: simulated effect size; keep 0 if you want to simulate null distribution\n",
    "    - bootstrap: make a bootstrapped sample before training\n",
    "    \"\"\"\n",
    "\n",
    "    assert delta == 0 or not russian, \"delta supported only in ITM\"\n",
    "\n",
    "    assert not bootstrap or not permute, \"bootstrap and permute are mutually exclusive\"\n",
    "\n",
    "    assert (\n",
    "        len(estimators) == 2\n",
    "    ), \"should provide exactly two base estimators in estimators\"\n",
    "\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "    target = russian_to_target[russian]\n",
    "\n",
    "    prediction_space_adj = prediction_space[\n",
    "        [\"type\"] + list(data.drop(columns=[target, \"type\"]).columns)\n",
    "    ]\n",
    "\n",
    "    model = Pipeline(\n",
    "        [\n",
    "            (\"ct\", ct),  # make sure ct's first column is type\n",
    "            (\"estimator\", DispatchEstimator(estimators)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if bootstrap:\n",
    "        data = bootstrap_data(data)\n",
    "\n",
    "    if permute:\n",
    "        type_new = stratified_permute(data[\"type\"], strats=permute_strats)\n",
    "    else:\n",
    "        type_new = data[\"type\"]\n",
    "\n",
    "    data_permuted = pd.concat(\n",
    "        [\n",
    "            type_new.reset_index(drop=True),\n",
    "            data.drop(columns=[\"type\"]).reset_index(drop=True),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    if delta != 0:\n",
    "        data_permuted.loc[data_permuted[\"type\"] == 0, target] += delta / 2\n",
    "        data_permuted.loc[data_permuted[\"type\"] == 1, target] -= delta / 2\n",
    "\n",
    "    model.fit(data_permuted.drop(columns=[target]), data_permuted[target])\n",
    "\n",
    "    if russian:\n",
    "        pred = model.predict_proba(prediction_space_adj)[:, 1]\n",
    "    else:\n",
    "        pred = model.predict(prediction_space_adj)\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_delta(\n",
    "    data,\n",
    "    prediction_space,\n",
    "    estimators,\n",
    "    number_of_permutations,\n",
    "    russian,\n",
    "    ct,\n",
    "    statistics=(identity,),\n",
    "    null_delta=0,\n",
    "    groupby_columns=(\"year_of_birth\",),\n",
    "    use_logodds=False,\n",
    "    iter_offset=0,\n",
    "    bootstrap=False,\n",
    "    seed=42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs permutation or bootstrap and calculates distribution of\n",
    "    imputed true difference (called delta) on prediction space\n",
    "    (usually full space of all possible values of our variables)\n",
    "\n",
    "    estimators is a pair of two sklearn's estimators\n",
    "\n",
    "    russian is bool (True/False)\n",
    "\n",
    "    ct is ColumnTransformer that makes preprocessing\n",
    "    (i.e. one hot encoding of categorical features)\n",
    "\n",
    "    To avoid memory issues, we keep only averaged values of delta\n",
    "    (or some function of delta, e.g. np.absolute: include them\n",
    "    into statistics if you need it)\n",
    "    across all variables\n",
    "    with except to ones mentioned in groupby_columns\n",
    "    (i.e. \"year_of_birth\" by default)\n",
    "\n",
    "    null_delta allows you to artifically impose some effect size\n",
    "    (not used in the paper)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    stat_names = [\"delta_\" + stat.__name__ for stat in statistics]\n",
    "\n",
    "    groupby_columns = list(groupby_columns)\n",
    "\n",
    "    r = []\n",
    "\n",
    "    predictions_futures = [\n",
    "        predict_de.remote(\n",
    "            data,\n",
    "            prediction_space,\n",
    "            estimators,\n",
    "            ct,\n",
    "            russian,\n",
    "            permute=not bootstrap,\n",
    "            delta=null_delta,\n",
    "            bootstrap=bootstrap,\n",
    "            seed=i + iter_offset + seed,\n",
    "        )\n",
    "        for i in range(number_of_permutations)\n",
    "    ]\n",
    "\n",
    "    predictions = ray.get(predictions_futures)\n",
    "\n",
    "    r = [\n",
    "        prediction_space[[\"type\"] + groupby_columns].assign(\n",
    "            pred=pred, iter=[it] * prediction_space.shape[0]\n",
    "        )\n",
    "        for it, pred in enumerate(predictions, start=iter_offset)\n",
    "    ]\n",
    "\n",
    "    results = pd.concat(r, axis=0).reset_index(drop=True)\n",
    "    results.columns = list([\"type\"] + groupby_columns) + [\"pred\", \"iter\"]\n",
    "\n",
    "    delta = (\n",
    "        get_delta(results, use_logodds=use_logodds)\n",
    "        .assign(\n",
    "            **{\n",
    "                stat_name: lambda x, stat=stat: stat(x[\"delta\"])\n",
    "                for stat_name, stat in zip(stat_names, statistics)\n",
    "            }\n",
    "        )[groupby_columns + stat_names + [\"iter\"]]\n",
    "        .groupby(groupby_columns + [\"iter\"])\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_wrap(\n",
    "    f, number_of_permutations, *args, permutations_per_iteration=1000, **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Helper function to split large task into several smaller\n",
    "    \"\"\"\n",
    "    assert number_of_permutations % permutations_per_iteration == 0\n",
    "\n",
    "    number_of_splits = number_of_permutations // permutations_per_iteration\n",
    "    return pd.concat(\n",
    "        [\n",
    "            f(\n",
    "                *args,\n",
    "                number_of_permutations=permutations_per_iteration,\n",
    "                iter_offset=i * permutations_per_iteration,\n",
    "                **kwargs\n",
    "            )\n",
    "            for i in tqdm(range(number_of_splits))\n",
    "        ],\n",
    "        axis=0,\n",
    "    ).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deltas_and_full_pred(\n",
    "    data,\n",
    "    estimators,\n",
    "    data_real,\n",
    "    data_cat,\n",
    "    number_of_permutations,\n",
    "    russian,\n",
    "    null_delta=0,\n",
    "    permutations_per_iteration=1000,\n",
    "    bootstrap=False,\n",
    "):\n",
    "\n",
    "    target = russian_to_target[russian]\n",
    "\n",
    "    data = data[data_real + data_cat + [target]]\n",
    "\n",
    "    ct = ColumnTransformer(\n",
    "        [(\"real\", \"passthrough\", data_real), (\"catenc\", OneHotEncoder(), data_cat)],\n",
    "        sparse_threshold=0,\n",
    "    )\n",
    "\n",
    "    full_pred = full.assign(\n",
    "        pred=lambda x: predict_de._function(\n",
    "            data, x, estimators, ct, russian, permute=False, seed=42\n",
    "        )\n",
    "    )\n",
    "\n",
    "    def get_deltas(bootstrap):\n",
    "        return concat_wrap(\n",
    "            permutation_delta,\n",
    "            number_of_permutations=number_of_permutations,\n",
    "            permutations_per_iteration=permutations_per_iteration,\n",
    "            data=data,\n",
    "            prediction_space=full,\n",
    "            estimators=estimators,\n",
    "            russian=russian,\n",
    "            ct=ct,\n",
    "            null_delta=null_delta,\n",
    "            statistics=(identity, np.abs),\n",
    "            use_logodds=russian,\n",
    "            bootstrap=bootstrap,\n",
    "        ).rename(columns={\"delta_identity\": \"delta\"})\n",
    "\n",
    "    return get_deltas(bootstrap=False), get_deltas(bootstrap=True), full_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculations\n",
    "These are values used in the paper. You can decrease `number_of_permutations` to 1000 to save time. `permutation_per_iterations` is used to save memory: decrease it if you have memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_permutations = 10000\n",
    "permutations_per_iteration = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ITM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_params(dct):\n",
    "    q = {re.sub(\"^estimator__\", \"\", k): v for k, v in dct.items()}\n",
    "    q[\"random_state\"] += 1\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"itm_cv_model_select.pickle\", \"rb\") as f:\n",
    "    itm_tuning = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "itm_params = (\n",
    "    itm_tuning.query('estimator == \"GradientBoostingRegressor\"')\n",
    "    .set_index(\"type\")[\"cv_best_params\"]\n",
    "    .apply(prepare_params)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3cec642f013430fb145d08c00aa2e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e640224f124dc185b27021d02b5b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "delta_ITM_perm, delta_ITM_bootstrap, pred_ITM_full = get_deltas_and_full_pred(\n",
    "    data=data_ITM,\n",
    "    estimators=[GradientBoostingRegressor(**itm_params[i]) for i in (0, 1)],\n",
    "    data_cat=[\"mother tongue\", \"residence\", \"sex\"],\n",
    "    data_real=[\n",
    "        \"type\",\n",
    "        \"year_of_birth\",\n",
    "        \"language population\",\n",
    "        \"elevation\",\n",
    "        \"village population\",\n",
    "    ],\n",
    "    number_of_permutations=number_of_permutations,\n",
    "    permutations_per_iteration=permutations_per_iteration,\n",
    "    russian=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_ITM_perm.to_csv(\"delta_itm_perm_gbr_splitted.csv\", index=False)\n",
    "delta_ITM_bootstrap.to_csv(\"delta_itm_bootstrap_gbr_splitted.csv\", index=False)\n",
    "pred_ITM_full.to_csv(\"pred_itm_full_gbr_splitted.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"russian_cv_model_select.pickle\", \"rb\") as f:\n",
    "    russian_tuning = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "russian_params = (\n",
    "    russian_tuning.query('estimator == \"GradientBoostingClassifier\"')\n",
    "    .set_index(\"type\")[\"cv_best_params\"]\n",
    "    .apply(prepare_params)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861a454c209c4405806738d3adb37a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faee890260394c519de2b98f22f00ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    delta_russian_perm,\n",
    "    delta_russian_bootstrap,\n",
    "    pred_russian_full,\n",
    ") = get_deltas_and_full_pred(\n",
    "    data=data_russian,\n",
    "    estimators=[GradientBoostingClassifier(**russian_params[i]) for i in (0, 1)],\n",
    "    data_cat=[\"mother tongue\", \"residence\", \"sex\"],\n",
    "    data_real=[\n",
    "        \"type\",\n",
    "        \"year_of_birth\",\n",
    "        \"language population\",\n",
    "        \"elevation\",\n",
    "        \"village population\",\n",
    "    ],\n",
    "    number_of_permutations=number_of_permutations,\n",
    "    permutations_per_iteration=permutations_per_iteration,\n",
    "    russian=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_russian_perm.to_csv(\"delta_russian_perm_gbr_splitted.csv\", index=False)\n",
    "delta_russian_bootstrap.to_csv(\"delta_russian_bootstrap_gbr_splitted.csv\", index=False)\n",
    "pred_russian_full.to_csv(\"pred_russian_full_gbr_splitted.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
